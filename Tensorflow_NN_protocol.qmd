---
title: "Sequential Neural Network Modeling Protocol"
author: "Turtles"
date: 3/07/2024
format:
  html:
    code-fold: true
jupyter: python3
---


# Introduction

This document presents a protocol (e.g., set of guidelines) for building reliable predictive engineering models with sequential neural networks.   The protocol is specific to engineering because XXXX.
 
# Data model description
 

Tensorflow.keras.models  import Sequential 


# Model parameters

## Layers

    Dense layers (Bartu)
This medium link will explain the logical and mathematical background of dense layers very well.
https://www.google.com/url?sa=i&url=https%3A%2F%2Fmedium.datadriveninvestor.com%2Fneural-network-simplified-c28b6614add4&psig=AOvVaw3vJtbw0bV02NURHOEBcK2L&ust=1711712237162000&source=images&cd=vfe&opi=89978449&ved=0CBIQjRxqFwoTCMitzZDvloUDFQAAAAAdAAAAABA9

Computational Neural Networks aren't only about that, there is a whole different inside it.
1. Connecting the number of dense layers and their widths with an equation using a for loop is very usable in many cases because in many cases it causes the final error rate to decrease. The systematic nature of equations make those kind of neural networks much more stable and well-organized. Which a basic example of this case will look something like this:
  model = Sequential()
  constant = # some constant number
  for th_layer in range(number_of_layer+1, 1, -1):
    model.add(th_layer * constant)
  ...
  and a basic hyperparameter optimization (such as random search) will look like this:
  turns = different values for constant
  for i in range(turns):
    constant = random.randint(constant_min, constant_max) # you can check if it haven't been tried.
    model = Sequential()
    for th_layer in range(number_of_layer+1, 1, -1):
      model.add(th_layer * constant)
    ... save the error of models in order to understand the best model overall, this process is applicable several times( same trials for all constants) to ensure the best constant value.
2. That equation doesn't have to be simple, in fact, it can be really complex or it can contain high-level mathematical operation (such as logarithms, exponentials, derivatives, or more). For example:
  Width = base ** (total_layerno - .th_layer) # where the base can be a float (you have to round in that case).


## Activation
Activation function is a mathematical function that determines how the weighted sum of inputs is transformed into an output from a neuron, which plays a vital role in enabling neural networks to learn complex patterns in data.
Nonlinearity can be introduced to neural networks through the activation function. For example, when the activation function is nonlinear, a two-layer neural network can be proven to be a universal function approximator.
The suitable choice of activation function for a given task can be determined through experimentation and general experience.
Common Activation Functions:
o	Rectified Linear Unit (ReLU): The most widely used activation function. It returns the input value for positive inputs and zero for negative inputs (max(0, x), where x is the input). It helps overcome the vanishing gradient problem, and it’s widely used in hidden layers to introduce nonlinearity.
o	Leaky ReLU: A modified ReLU that allows a small non-zero output for negative inputs, which is a gradual transition for negative inputs (for positive input values, it behaves like the ReLU function, returning input (x); for negative input values, it returns a small negative value proportional to the input (x), determined by the negative slope parameter). 
Leaky ReLU is more stable than ReLU, especially when dealing with large gradients. In standard ReLU, neurons can become inactive (output zero) for negative inputs during training. Leaky ReLU addresses this by allowing a small non-zero output for negative inputs. The non-zero gradient for negative inputs helps prevent vanishing gradients during backpropagation.
o	Sigmoid (Logistic): Maps input values to a range between 0 and 1 (defined as 1 / (1 + exp(-x))). Commonly used in binary classification problems.
o	Tanh (Hyperbolic Tangent): Similar to sigmoid but maps inputs to a range between -1 and 1.
o	Softmax: Converts a vector of values into a probability distribution. Elements of the output vector are in the range [0, 1] and sum to 1 (exp(x) / sum(exp(x)) for each vector element). Often used in the last layer of classification networks. Commonly used in multi-class classification problems.
o	Linear: Simplest activation function. Maps input values as they are (f(x) = x). Commonly used in the output layer of a neural network, especially in regression tasks. 

Szandała, T. (2021). Review and comparison of commonly used activation functions for deep neural networks. Bio-inspired neurocomputing, 203-224.
Apicella, A., Donnarumma, F., Isgrò, F., & Prevete, R. (2021). A survey on modern trainable activation functions. Neural Networks, 138, 14-32.
Dubey, S. R., Singh, S. K., & Chaudhuri, B. B. (2022). Activation functions in deep learning: A comprehensive survey and benchmark. Neurocomputing, 503, 92-108.
Rasamoelina, A. D., Adjailia, F., & Sinčák, P. (2020, January). A review of activation function for artificial neural network. In 2020 IEEE 18th World Symposium on Applied Machine Intelligence and Informatics (SAMI) (pp. 281-286). IEEE.


## Kernel
(Charlie)
Kernel_regularizers = L!(0.005) 

Number of dense layers 

## Dropout

(Arin)
Dropout (XX) - designed to prevent overfitting, randomly drop out a proportion of neurons 



# Model compilation
 
Model.compile 

## Optimizers

Optimizer karas.optimizers.legacy (learning rate!) 

Loss "mean_squared_error", or "abs error" 

## Metrics

## Epochs

## Batch size

## Validation data

Validation_data (training data), tuning hyperparameters 

# Example

```{{python}}

import pandas as pd
import tensorflow as tf
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error
from sklearn.preprocessing import RobustScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout, LeakyReLU
from keras.regularizers import L1
from keras.callbacks import Callback, ModelCheckpoint
from keras.optimizers.schedules import ExponentialDecay
import keras
from keras import backend as K

class PrintLearningRate(Callback):
    def on_epoch_end(self, epoch, logs=None):
        lr = self.model.optimizer.lr
        if self.model.optimizer._decayed_lr:
            lr = self.model.optimizer._decayed_lr(tf.float32).numpy()
        print(f"Learning rate for epoch {epoch + 1} is {lr}")

def apply_noise_reduction(data, window_size=3):
    return data.rolling(window=window_size, min_periods=1).mean()

dfmain = pd.read_csv("../merged.csv")
dfmain = dfmain.drop(columns=["Unnamed: 0"], axis=1)
#dfmain = dfmain.sample(frac=0.5)
dfmain["Corr1"] = dfmain["A"] * dfmain["fy"]
dfmain["Corr2"] = dfmain["I"] * dfmain["fy"]
scaler = RobustScaler()
y = dfmain["Pn"]
x = dfmain.drop(columns=["Pn"], axis=1)
x = x.drop(columns=["section"], axis=1)
x_tr = x.iloc[:850000].values
x_te = x.iloc[-100000:].values
y_tr = y.iloc[:850000].values
y_te = y.iloc[-100000:].values
x_val = x.iloc[850000:-100000]
y_val = y.iloc[850000:-100000].values
batchsize = 48

epochsno = 200
x_tr = scaler.fit_transform(x_tr)
x_te = scaler.transform(x_te)
x_val = scaler.transform(x_val)
learning_ratei = 0.0012
learning_ratef = 0.0004

# depict the layer structure

decaysteps = round(900000 / batchsize)
print("decaysteps",decaysteps)
targetlr = 0.0004
initiallr = 0.0015
decayrate = round((targetlr/initiallr) ** (1/epochsno), 4)
print(decayrate)
lr_schedule = ExponentialDecay(
    initial_learning_rate=initiallr,
    decay_steps=decaysteps,
    decay_rate=decayrate,
    staircase=True)

class RegressionEarlyStopping(Callback):
    def __init__(self, monitor, percentage_delta, patience, verbose, mode):
        super(RegressionEarlyStopping, self).__init__()
        self.monitor = monitor
        self.percentage_delta = percentage_delta
        self.patience = patience
        self.verbose = verbose
        self.mode = mode
        self.wait = 0
        self.stopped_epoch = 0
        self.best = None
        self.best_weights = None

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if current is None:
            return

        improvement = 0
        if self.best is not None:
            if self.mode == 'min':
                improvement = (self.best - current) / self.best
            elif self.mode == 'max':
                improvement = (current - self.best) / self.best

        if self.best is None or \
                (self.mode == 'min' and improvement > self.percentage_delta) or \
                (self.mode == 'max' and improvement > self.percentage_delta):
            self.best = current
            self.wait = 0
            self.best_weights = self.model.get_weights()
        else:
            self.wait += 1
            if self.wait >= self.patience:
                self.stopped_epoch = epoch
                self.model.stop_training = True
                self.model.set_weights(self.best_weights)
                if self.verbose > 0:
                    print(f"Epoch {epoch + 1}: early stopping based on {self.monitor}")

    def on_train_end(self, logs=None):
        if self.stopped_epoch > 0 and self.verbose > 0:
            print(f"Restored model weights from the end of the best epoch: {self.stopped_epoch + 1}.")


modelmse = []
K.clear_session()
model_checkpoint = ModelCheckpoint(filepath="fourth.csv",save_best_only=True, monitor='val_loss', mode='min', verbose=1)
early_stopping2 = RegressionEarlyStopping(
    monitor='val_loss',
    percentage_delta=0,
    patience=30,
    verbose=1,
    mode='min'
)
model = Sequential(name=f"final")
moptimizerA = keras.optimizers.legacy.Adam(learning_rate=lr_schedule)
for i in range(6, 0, -1):
    model.add(Dense(10 * i , activation=LeakyReLU(0.04), kernel_regularizer=L1(0.005)))
    if i % 2 == 0 and i != 0:
        model.add(Dropout(round(0.12*(i/(i+1)))))
    #if i // 3 == 0 and i != 0: model.add(Dropout(rate=0.1))
model.add(Dense(1, activation="linear"))
model.compile(optimizer=moptimizerA, loss="mean_squared_error", metrics=['mean_squared_error'])
model.fit(x_tr, y_tr, epochs=epochsno, batch_size=batchsize, validation_data=[x_val, y_val], callbacks=[early_stopping2, PrintLearningRate(), model_checkpoint])
predictions = model.predict(x_te)
modelmse = mean_squared_error(y_te, predictions)
modelmspe = mean_absolute_percentage_error(y_te, predictions) ** 2
K.clear_session()
print(modelmse)
print(modelmspe)

```
